# 基于FPGA的卷积神经网络实现

## 1. 简介

<https://blog.csdn.net/qq_38798425/article/details/106359726>

本项目是一个教学demo，很多地方为了快速完成代码风格比较差劲，在后面会慢慢进行完善和更新。

事先声明，仅用于记录和讨论，有任何问题欢迎批评指正，只是觉得菜的大佬们请绕路，就不用在这里说大实话了，因为本身就是一个粗糙的demo。
提起把项目代码讲解一下的念头主要是源于最近一个同学开始转行做这个，感觉很复杂无从下手。在给他讲的时候想起我最开始入手一块FPGA到自己写出整个框架的艰辛，以及寻找并阅读开源项目的艰难，所以打算把整个过程完整的记录下来，也方便和大家交流。

现在github上对于在FPGA实现卷积神经网络的项目也很多，各大公司也都有做，Xilinx和AMD的开源项目也能搜到，但是由于功能太强大而导致代码复杂新手很难阅读（也许这是FPGA口的一个特色？）。另外就是有很多论文，各种各样的加速方式，看了之后，woc NB，但是咋复现啊。本项目由于主要是从结构出发，因此只是对结构进行了实现，对于硬件层面的优化较少，都是使用最简单的方式进行实现的，因此可能理解起来也算是容易一些。当然现在越来越多的研究者和同学使用HLS进行开发，确实HLS相对于简单很多，开发周期大大缩短。

对于基于FPGA实现CNN的研究，个人感觉单纯的在这方面进行普适性的研究已经许久没有颠覆性的进步了，研究人员在FPGA上面实现了VGG、Resnet等各种各样的网络结构，各种各样的硬件级优化也都做了很多。而我们从工程应用的角度把他作为一种工具去在某一个领域去进行加速，这样是可以针对特定的应用场景特定的网络进行特定的处理。很多现在的论文也是这样，做一个项目场景，而FPGA的使用是为了加速这一过程，至于CNN，那也只是一种方法而已。
从工程角度，对于使用FPGA加速CNN，个人觉得应该分为四个步骤：

-    在软件层面对CNN进行优化
-    在软件层面对CNN进行FPGA适应性处理
-    将CNN算法分解为适合FPGA实现的结构
-    FPGA实现及优化

这整个应该是一个完整的流程。

首先FPGA本身的存储空间有限，现在的大型网络具有巨大的参数量，哪怕是在一些具体应用中使用了较小的网络（如本项目，因为要实现的功能并不复杂，因此网络结构也比较简单），参数量仍然很大，因此第一步应该在软件层面就优化一些，比如剪枝，既可以减少参数量又可以减少计算量，何乐不为。

其次CNN不一定完全适合FPGA，比如我们希望在FPGA中调用DSP单元进行定点数计算，那么我们需要对CNN的参数进行定点数量化处理。

分解的话主要是提取CNN的计算流程，把每一步的计算公式完整的使用matlab或者python等进行实现，而且不掉调用库函数，完全来仿真FPGA的实现，比如卷积是否要分解为多步进行等。

最后才是在FPGA上的实现以及优化。

个人意见哈~也不见得对，而且仅限于单片处理，多片的话要考虑的东西就更多了。下篇开始我将会对这个项目按照我个人写程序时候的顺序进行讲解。所有的.v文件我已经上传到了github。

[基于FPGA的卷积神经网络实现](https://github.com/MasLiang/CNN-On-FPGA)

之前做的一个[讲解视频](https://www.bilibili.com/video/BV1yT4y1c7Uu/)。

## 2. 框架

<https://blog.csdn.net/qq_38798425/article/details/106723877>

首先设计整体框架，再根据整体结构去设计每一个子模块，通过例化子模块来形成完整的工程程序。也就是基于FPGA的卷积神经网络实现里面的top.v文件。

本项目要实现的神经网络结构如下图所示：

![](https://img-blog.csdnimg.cn/20200612215813261.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4Nzk4NDI1,size_16,color_FFFFFF,t_70#pic_center)

### 2.1 本项目实现的整体网络结构

对于在FPGA当中实现神经网络而言，每一种功能的层都单独设计一个子模块。而使用这些子模块搭建一个完整的网络框架主要有两种方法：

-    每一层都单独进行例化，占用单独的硬件资源。
-    同样功能的层复用同一部分硬件资源，通过parameter来进行区分

本项目为了初学者放心食用选择比较简单的第一种方式，当然前提是硬件资源足够多的，如果实现一个较大型的网络这样的方式是不可能的，还是需要使用第二种方法，不过偶从入门来说，第一种方法是非常容易理解的。

在最开始先要设置一个时钟模块，调用片上PLL，也就是clocking Wizard IP核。我使用的片上输入时钟为50M，输出时钟设置为两路，一路为原本的50M时钟，用于和系统时钟相关的地方（当然这个不一定会的上），另一路先随意设置为200M，在后面实际使用的时候再进行调整。

![时钟模块设置（1）](https://img-blog.csdnimg.cn/20200612214826777.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4Nzk4NDI1,size_16,color_FFFFFF,t_70#pic_center)

![
时钟模块设置（2）](https://img-blog.csdnimg.cn/20200612214912190.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4Nzk4NDI1,size_16,color_FFFFFF,t_70#pic_center)

设置好后在top文件中进行例化

```verilog
clock_pll clock_control
    (
        .CLK_IN1        (clk_in)		,
        .CLK_OUT1       (clk_50)		,
        .CLK_OUT2       (clk_200)		,
        .RESET          (~rst_n)		,
        .LOCKED         ()
    );
```

那么这样一来我们就拥有一个快速的时钟，能够让我们的卷积神经网络飞速运行起来了。

下面我们需要一些子模块了，根据整个网络的结构，我们需要设计输入数据存储层、参数存储层、卷积层、池化层、全连接层，当然，还需要层间的缓存层。对于一个卷积层参数与输入数据需要同时进入，因此可以将参数存储层和层间输入缓存层合并为同一个模块。

以卷积层为例，先写出大概的框架，后面再慢慢填补。

```verilog
module Layer1_Conv_Top#(
	parameter	filter_size=5		
	)
	(
	input           clk_in,
	input           rst_n,
	input           data_in,
	input	        weight_in,
	output	  reg 	data_out
    );
    data_out <= data_in*weight_in;
endmodule
```

各个子模块创建完毕后，再去top文件里面例化一下并串联在一起，一个完整的框架就搞定了。

```verilog
pic_in layer0(	
	.clk_in			(clk_200)	,
	.rst_n			(rst_n)		,					
	.map			(l1d1)		,				
	.ready			(l1r1)		,
	.weight			(l1w1)		
    );
Layer1_Conv_Top layer1_conv(
	.clk_in			(clk_200)	,
	.rst_n			(rst_n)		,
	.data_in		(l1d1)		,
	.weights		(l1w1)		,
	.data_out		(l1d2)		
    );
Layer1_Pool_Top layer1_pool
	(
	.clk_in			(clk_200)	,	
	.rst_n			(rst_n)		,
	.data_in		(l1d2)		,
	.data_out		(l1d3)	
    );
pool1_out_buffer layer1_pool_buffer(
	.clk_in			(clk_200)	,
	.rst_n			(rst_n)		,
	.data_in		(l1d3)		,
	.data_out		(l2d1)		,
	.weight			(l2w1)
    );
	 
//layer2

Layer2_Conv_Top layer2_conv(
	.clk_in			(clk_200)	,
	.rst_n			(rst_n)		,
	.data_in		(l2d1)		,
	.weights		(l2w1)		,
	.data_out		(l2d2)	
    );
Layer2_Pool_Top layer2_pool
	(
	.clk_in			(clk_200)	,	
	.rst_n			(rst_n)		,
	.data_in		(l2d2)		,
	.data_out		(l2d3)		
    );
pool2_out_buffer layer2_pool_buffer(
	.clk_in			(clk_200)	,
	.rst_n			(rst_n)		,
	.data_in		(l2d3)		,
	.data_out		(l3d1)		,,
	.weight			(l3w1)
    );

//layer3

Layer3_Conv_Top layer3_conv(
	.clk_in			(clk_200)	,
	.rst_n			(rst_n)		,
	.data_in		(l3d1)		,
	.weights		(l3w1)		,
	.data_out		(l3d2)	
    );
conv3_out_buffer layer3_conv_buffer
	(
	.clk_in			(clk_200)	,
	.rst_n			(rst_n)		，
	.data_in		(l3d2)		,
	.data_out		(l3d3)	
    );
Layer3_Pool_Top layer3_pool
	(
	.clk_in			(clk_200)	,	
	.rst_n			(rst_n)		,
	.data_in		(l3d3)		,
	.data_out		(l4d1)		
    );

//full_connection
ful_conn  layer4_full_connect(
	.clk_in		(clk_200),
	.rst_n		(rst_n),
	.data_in	(l4d1),
	.data_out	(Dout)
    );
```

程序中用来连接各个子模块的wire类型数据的命名规则是

```verilog
l（layer）+第几层+d（data）+第几个模块
```

到现在一个整体的网络框架应该是搭建完毕了。这里有个很值得注意的地方在于，为了编写简单，我将同样功能的模块都单独写了子模块，这样对于他们的不同就可以一个一个去处理。但是事实上，他们的整体思路是一样的，只需要改一些参数，也就是说通过设置许多的parameter也可以达到同样的目的。在所有的子模块介绍完后我将针对这一问题单独进行讲解。

## 3. 资源分配

<https://blog.csdn.net/qq_38798425/article/details/107084504>
<https://blog.csdn.net/qq_38798425/article/details/107116312>

前面曾经说过这个demo使用最简单的方法，也就是对每一层都单独分配资源，所以需要提前设计好每一层所分配的资源数量。这里的资源主要是指DSP单元和片上存储资源,CNN的卷积层是计算密集型层，全连接层是存储密集型层，整个CNN所需要的计算资源（也就是我们使用DSP单元来实现）和存储资源远远大于对其他资源的需求。仍然是从最简单的方式开始，整个demo的网络结构很小而简单（网络结构在这里），再买上一块儿略屌的板子，比如说zynq7000-zc706（这板子确实很贵，但是一开始我们可以只是仿真，对于上板可以等基本掌握了这种设计思路之后，再进行优化，然后对于不同的板子进行不同的设），19.1M block RAM，900DSP单元（详细信息）。现在网络框架有了板子也有了，那么我们根据这些资源数据来设计。

整个网络结构的参数量如下表所示：

层 | 参数量（个）
-|:-:|-
卷积层1 | 5 * 5 * 1 * 4
卷积层2 | 5 * 5 * 4 * 8
卷积层3 | 5 * 5 * 8 * 16
全连接层1 | 4032 * 100
全连接层2 | 100 * 4
全总计 | 407700

假设我们使用全精度（32位浮点）来存储，那么总计需要片上存储空间13M（如果对参数进行低位宽量化，将会减小对存储的需求）。

下面考虑可能出现的运行缓存，这里的缓存是指，在每一层计算的时候，该层的结果在输入到下一层使用之前需要暂时存储起来。我们先不考虑优化问题，假设每一层的所有输出都需要缓存起来，并且一直不释放一直到整个前向传播过程结束，来计算最大需要的缓存空间。（事实上，每一部分缓存数据在使用完后可以得到释放，只需要单层最大缓存空间即可；另外，使用一定的优化方法可以进一步降低缓存占用空间）。每一层输入所需要的缓存空间如下表所示：

层 | 输入缓存空间
-|:-:|-
卷积层1 | 256 * 100 * 1
卷积层2 | 126 * 48 * 4
卷积层3 | 61 * 22 * 8
全连接层1 | 4032
全连接层2 | 100
全总计 | 64660

仍然假设使用全精度（32位浮点数）来存储，那么总计需要片上存储空间2M。是不是就结束了？其实并没有，前面我们只是认为卷积层和全连接层这样的计算层需要缓存，实际上，池化层也需要单独拎出来，也就是说池化层的输入也需要缓存，每一个卷积层后面会接一个池化层，那么我们再重新来计算一下存储需求上限：

层 | 输入缓存空间
-|:-:|-
卷积层1 | 256 * 100 * 1
池化层1 | v252 * 96 * 4
卷积层2 | 126 * 48 * 4
池化层2 | 122 * 44 * 8
卷积层3 | 61 * 22 * 8
池化层3 | 56 * 18 * 16
全连接层1 | 4032
全连接层2 | 100
全总计 | 64660

这样一来缓存需要的存储空间上限为6.7M，也就是说，如果我们将所有的参数和缓存数据存储在片上存储空间，总计需要19.7M，ZC706是不可以cover的。但是呢这里有个值得注意的地方在于，我们所有的数据都是使用32bit的位宽进行存储，实际上我们可以使用更低bit来存储这些数据，这将缩小对空间的使用。关于量化的问提，进来的研究有很多，16bit定点数基本可以是无损的，pytorch1.4也提供了定点数量化工具，如果想在这方面进行研究的同学可以自行搜索一些相关论文（我有同学做这个，今年在CVPR2020有几篇低bit量化的工作做的已经很强了）。如果想快速做一个demo出来，最简单的办法就是训练出来的网络，将参数直接进行量化，不再retrain，这样会有一定精度损失。

回到FPGA的硬件资源继续分析，既然16bit定点数来存储各种数据我们认为是无损的，那么我们可以认为整个demo的存储需求上限是9.85M，也就是说这个板子能够cover整个网络的存储，所以就不需要在认为去分配每一层使用多少。这里要注意的是，我们说的是存储上限，也就是说实际设计的时候还会比这更低，这需要结合DSP资源的分配来设计结构，我们后面再进一步分析。

下面去考虑DSP单元的数量。DSP单元用于计算卷积，我们使用DSP计算乘法（乘加）运算，我们还是以最简单的来，就让一个DSP单元计算在一个时钟内计算一次乘法，加法再单独计算。

对于每个卷积核的卷积的计算有两种方式：

-    一些（或一个）DSP单元重复利用，每个时钟进行一定数量的（或一个）乘法运算，串行完成所有计算
-    多个DSP单元在一个时钟内完成所有乘法计算

对于不同的卷积核之间又有两种情况：

-    一层所有的卷积核并行进行
-    部分卷积核（也可能是一个卷积核）并行进行

对于本demo来说，如果我们在卷积核内选择第二种方式，卷积核间也选择第一种方式，那么每一层需要的DSP单元数量如下表所示：

层 | DSP单元数量（个）
-|:-:|-
卷积层1 | 5 * 5 * 1 * 4
卷积层2 | 5 * 5 * 4 * 8
卷积层3 | 5 * 5 * 8 * 16

是不是非常眼熟？没错，这就是前面的卷积层参数量的表格，显然这种方式等价于我们给每个卷积层参数分配一个DSP单元。也就是说我们总计需要4100个DSP单元，显然这是远远超过板载DSP单元数量的，因此这是不可行的。那么我们来考虑降低卷积核之间的并行程度，降到最低的状态即为每一层都只并行计算一个卷积核，那么每一层需要的DSP数量变为：

层 | DSP单元数量（个）
-|:-:|-
卷积层1 | 5 * 5 * 1
卷积层2 | 5 * 5 * 4
卷积层3 | 5 * 5 * 8

总计为925个，仍然是超过了板载资源上限。

因此我们的卷积核内部不能够全部并行进行，只能部分并行进行。

另一种组合是卷积核见仍然是全并行的，而卷积核内部是部分并行的，这种方式是可行的，这种方式所需要的最小DSP单元个数是每个卷积核分配一个DSP单元，那么总计需要28个，我们仍可以进一步的增加卷积核内部的并行程度，以充分利用板载DSP资源。

最后一种组合方式是卷积核内部部分并行，卷积核见也部分并行，显然这种方式的最小DSP单元需求量更小，也是可以考虑的一种。

结论：我们在分配DSP单元的时候，应使得每一层卷积核内部的卷积运算部分并行，卷积核之间部分或完全并行

下一节我将对这两种方式进行对比，并将对缓存的优化纳入考量，最终确定DSP资源的分配。

---

上一节我们分析了demo需要的存储空间上限和DSP单元的分配可能方案，这一节我们将前面提到的可行的方案进行比较，并将存储空间的优化考虑进来。

首先这四种可能的DSP单元分配方案方案列举一下：

-    卷积核内全并行，卷积核间全并行（DSP资源不足）
-    卷积核内全并行，卷积核间部分并行（DSP资源不足）
-    卷积核内部分并行，卷积核间全并行（可行）
-    卷积核内部分并行，卷积核间部分并行（可行）

现在我们来比较方案3和4。

首先他们的共同点是卷积核内部分并行，因此我们从这里开始分析，明确卷积核内部如何部分并行。
首先来看卷积的计算公式：
o u t ( x , y , n o u t ) = ∑ n i n N i n ∑ k x = 0 k − 1 ∑ k y = 0 k − 1 w n o u t × i n ( x + k x , y + k y , n i n ) + b ( n o u t ) out(x,y,n_{out}) = \sum^{N_{in}}_{n_{in}}\sum^{k-1}_{k_x=0}\sum^{k-1}_{k_y=0}w_{n_{out}}\times in(x+k_x,y+k_y,n_{in})+b(n_{out}) out(x,y,nout​)=nin​∑Nin​​kx​=0∑k−1​ky​=0∑k−1​wnout​​×in(x+kx​,y+ky​,nin​)+b(nout​)

这里的 n o u t n_{out} nout​是指输出的通道， n i n n_{in} nin​是指输入的通道，这个公式得到的经过卷积操作后的一个像素。公式中有三个 ∑ \sum ∑符号，对于一个求和符号而言，我们可以将被求和的数据并行计算完，再进行求和，也就是说我们可以将这个公式拆解为三层：

-    y方向的所有乘法并行进行并求和
-    x方向的所有乘法并行进行并求和
-    输入通道间的所有乘法并行进行并求和

如果三种同时满足就是我们前面提到的卷积核内部全并行的方式，但是这显然是不可行的。为了整齐性（也就是逻辑简单容易实现），我们的并行结构是在这里面选取的，如果选择1或者2中的一个，与3进行组合，那么在本demo中各层的每一个卷积核需要的DSP单元数量如下表所示：

层 | 单个卷积核的DSP需求
-|:-:|
卷积层1 | 5
卷积层2 | 20
卷积层3 | 40

这种方式我们假设卷积核间全串行，那么DSP需求下限为65个，是可以满足的。如果只选取3的话，也就是输入通道内全串行，输入通道间全并行，那么在本demo中各层的每一个卷积核需要的DSP单元数量如下表所示：

层 | 单个卷积核的DSP需求
-|:-:|-
卷积层1 | 1
卷积层2 | 4
卷积层3 | 8

这种方式显然板子是能够cover的，那么我们在此基础上去考虑卷积核间的并行程度。这时候我们来结合缓存进行分析。
首先我们来看一个卷积层的计算流程，这里贴一张传统老图，图片来自[这里](https://blog.csdn.net/BaiHuaXiu123/article/details/72190093)，这是一个单输入通道卷积核大小为4*4的卷积操作

![](https://img-blog.csdnimg.cn/20200704143316655.gif)

我们可以看到在计算得到第一个像素的时候，我们只需左上角的4 * 4个数据就好，另外很显然我们可以看到的是，随着卷积核对应位置的移动，每次计算结束后，左上角的数据将不会再被使用，也就是说可以释放这部分存储空间。对于多输入通道的卷积，也就是3D卷积，我们需要的只是对于一个3D输入的左上角部分 4 * 4 * 输入通道数个数据。如下图所示

![3D卷积占用的数据](https://img-blog.csdnimg.cn/20200704145248764.jpg#pic_center)

所以说我们在缓存这部分数据的时候，当满足了这一层的计算要求，我们就可以开始这一层的计算。按照前面动图的顺序执行流程，我们只是需要存储前三行和第四行的前四个数据，就可以开始这一层的计算，而由于每次计算，位于左上角的数据就不会再被利用了，因此新来的数据可以直接覆盖在这个位置。也就是说，只要我当前层计算的速度足够快，上一层的输出速度就追不上我，这样一来缓存空间就只剩下了一小部分。

以上的分析我们基于连续的两个卷积层，实际上，如果这里是卷积层+池化层，池化层+卷积层，这一结论仍然成立。

这样的方式当然不是永远能够使用，前提是所需要缓存的数据的前后两层卷积层必须占用不同的硬件资源可以同时进行计算。当然我们这个demo是恰好完美的符合了这个要求的。

池化层前面的数据显然不用说了，池化的速度是非常快的，足以让前面的卷积层输出追不上，那么池化层的输出与卷积层的输出呢？因此这就又回到了我们DSP资源的分配上来，如何能够让后面的卷积层足够快，快到前面的层的输出追不上。

继续说存储空间，有没有办法进一步减小缓存空间呢？继续来分析前面的图，如果我能够一次性提供所有的左上角的这些数据~~是不是突然觉得这些数据不需要被缓存，爽的一批？可是这些数据还是要被缓存，因为只有左上角的数据后面不会被使用，除非步长大于等于卷积核尺寸。但是一般来说我们不会这样做，但是有一种层确实是这样做的——池化层。一般而言池化层会选择2 * 2的尺寸且步长为2，也就是说，如果我一次性给池化层能够有足够的数据进行一次池化，那么这些数据经过池化操作后可以被丢弃，也就是说这部分不再需要缓存，池化层前面的缓存空间就可以去掉了。

到这里存储空间的分析结束，我们来针对这部分分析分配DSP单元。这里我们的分配需要尽可能去满足两点：

-    能够让卷积的层同时输出足够池化层进行池化的数据
-    能够让后面的层计算速度足够快，使得前面层的输出可以只缓存部分

注意这里说的是尽可能，因为如果DSP单元不能满足，那就增大缓存嘛~反正现在这个板子能cover的。

对于要求1，我们的方法就是同时计算相邻的四组卷积，这样就可以一次性输出四个数据（当然一次输出十六个相邻的数据也是可以的，甚至这是有其他好处的，我们后面会提到），如下图所示：

![四组卷积并行](https://img-blog.csdnimg.cn/20200704170810873.jpg#pic_center)

当然对于多卷积核的层来说，需要所有卷积核对应的通道同时输出，不然有的通道输出了，有的通道没有输出，就会导致已经输出的数据仍需要缓存等待其他通道的数据，这就要求我们在卷积核之间是全并行的，因此前面DSP单元的分配方案就可以选出来了：

**卷积核内部部分并行，卷积核之间全并行**

为了满足要求2，我们需要分析一下相邻两层计算速度之间的关系。

以我们的demo为例，卷积层1的卷积核尺寸为 5 × 5 5\times5 5×5，输入通道数量为1，输出通道数量为4，由于前面提到我们需要让输入通道间全并行，而相邻多组卷积之间的计算时间是一样的，因此我们只需要去计算一组卷积所需要的时间。前面提到过我们使用DSP单元来例化成乘法器，一个时钟内一个乘法器可以计算一次乘法，而加法可以作为流水线跟随在乘法后面，时序图如下所示，最终的计算时间等于：乘 法 数 量 ÷ 分 配 D S P 数 量 + 1 

![卷积时序图](https://img-blog.csdnimg.cn/20200704172510803.jpg)

卷积是个流水线过程，乘法的进行无需等待加法的完成而可以连续进行，我们近似认为计算时间等于 

乘 法 数 量 ÷ 分 配 D S P 数 量

第一层每一次卷积的乘法数量为 5 × 5 5\times5 5×5,第二层每一次卷积的乘法数量为 5 × 5 × 4 5\times5\times4 5×5×4。也就是说，二者的区别在于输入通道的数量，所以如果说我们将所有层卷积核内每一个输入通道内部使用同样多的DSP单元，那么不同层之间每一组卷积所需要的卷积核数量的比值就等于输入通道的比值。

而由于我们会使得第一层卷积层同时进行相邻的4组卷积计算，才能够使得第二层卷积层接收到一个数据/每通道，因此两层的比值还需要乘4。
前面提到过，如果同时进行16组卷积并行计算，那么第二层卷积就会每次接收到4个数据/每通道，也就是说第二层卷积仍可以同时进行4组相邻的卷积，这样这一层的输出仍不需要缓存直接池化。

因此，假设我们一个输入通道内分配`x`个DSP单元，第一层需要 16 × 4 × 1 × x。这里的16指16组卷积并行，4指4个卷积核，1代表着1个输入通道。那么第二层需要 4 × 8 × 4 × x。

这里还有第三层卷积层，按照前面的要求的话，我们为了能够让池化层不再缓存，我们需要同时计算4组相邻的卷积，那么就需要第二层卷积层同时计算16组卷积，第一层偶同事计算64组卷积，这样一来对DSP单元的需求呈指数型增长，显然是不可以接受的，因此这样的方式最多能够针对相邻两层，再多会导致DSP资源的不足，所以就牺牲一些缓存空间来保证DSP数量的充足。
因此第三层需要DSP单元数量为：

1 × 16 × 8 × x

这样一来总计需要的DSP单元数量为：

320 × x

板载DSP单元共有900个，所以x的最大值为2，就是说，每个输入通道内分配两个DSP单元进行计算。

每个输入通道内进行的是 5 × 5 5\times5 5×5的2D卷积，共有奇数个乘法，如果提供2个DSP单元的话逻辑会稍微复杂一点点（真的只有一点点），所以本着一切从简的原则，我们只分配1个，（当然分配1个的好处是我们还可以买便宜一些的板子，节约money）。

言归正传，我们现在已经得到了每一层卷积层的DSP单元分配数量。我们再回过头来看看我们各个层次的并行情况。

-    输入通道内是串行的
-    输入通道间并行
-    卷积核间并行，也可以被称之为输出通道间并行。
-    层间算不算并行呢？反正是在某些时刻不同的层在同时进行

资源的分配大概的思路就是这样，针对不同的结构，不同的FPGA板载资源，来从不同的角度去考虑。

下一节开始将分别设计不同的功能模块。

## 4. 数据量化

<https://blog.csdn.net/qq_38798425/article/details/107423892>
<https://blog.csdn.net/qq_38798425/article/details/107451507>

这一节主要是软件层面的处理。之所以要进行数据的量化，一个是为了能够使用低bit来表达整个网络从而达到对其压缩的目的，减小对存储空间的需求；二就是FPGA中的DSP单元属于定点数DPS单元，更善于处理定点数运算。

定点数量化方面的研究有很多，基本上16bit不会对模型造成损失，使用8bit经过一些trick之后损失也很小，研究方面人们已经开始关注更低的位宽，但是目前在实现上，完全使用更低位宽确实会带来较大的损失，因此最佳的办法是使用动态的量化方式，也就是说对于不同的层不同的数据使用不同的量化位宽。动态量化方面也有很多研究，比如之前达摩院提出过一个使用很复杂的数学方法算出来每一层的最佳量化位宽（反正我是没看懂）。这种研究层面的东西不是我们的重点，我们的重点在于实现，仍然是demo的出发点：一切以简单为主，主要是来说明整个流程，所以我们先不去考虑每一层的最优量化位宽是多少，统一用一个较大的位宽（16bit）来处理，至于用更低的位宽和动态的量化位宽，那其实是软件层面的事情，去复现那些现有的研究就可以。当然如果有研究兴趣的同学，目前低bit量化是一个害挺热门的方向。

pytorch1.4开始提供了[8bit量化工具](https://pytorch.apachecn.org/docs/1.4/88.html)，有兴趣的同学可以探索一下，不过据说并不是很好用。

对数据进行量化最简单的方式就是将已经训练好的参数直接进行均匀量化和解量化。举个例子：

q(x)=round(x/scale+zero)

这是最常见的量化方式，也就是说先确定量化的上下界，也就能确定你所要表达的数据所在的位置，然后

round(x_max​/scale+zero)=q_max​

round(x_{min}/scale+zero)=q_{min}

zero代表着一个偏置，因为如果直接将 x x x进行放缩可能不能够恰好的占满整个量化区间的上下界，也就是不一定能够恰好满足上面两个式子。

使用这样的方式对输入数据`x`、权重`w`和偏置`b`进行量化之后，我们得到了`x_q`​、`w_q`​和`b_q`​，所以我们的计算公式变成了：

f_q(x)=w_q\otimes x_q+b_q

因此对于卷积层的输出我们还需要进行解量化，这就是一个很麻烦的事情，因为我们对于`x`、`w`、`b`使用的`zero`，`scale`都不同，这方面属于软件层面的trick，也有许多人去研究，我们暂时不做考虑如何能够更小误差的解量化，我们首先做一个trick来使得解卷积变得简单。

影响我们解量化误差大小的，主要在于对不同的数据存在不同的`zero`，那么如果我们去掉这一项，情况立马发生变化

q(x) = round(x/scale)

那么我们解量化的时候只需要

f(x)=f_q(x) \times scale_x \times scale_w

要注意的是，我们需要保证

scale_x\times scale_w = scale_b

而此时引入的量化误差，仅仅来自于在对数据进行量化的时候使用取整函数带来的误差，相对来说要小了很多。

当然这样做的问提也是很显著的，我们的量化区间并没有被完整的利用。我们这样做的`scale`将由以下公式决定：`

round(x_{max}/scale)\leq q_{max}

round(x_{min}/scale)\ge q_{min}

也就是说在绝大多数情况下我们只能满足上下界中的一个。而不能完整利用量化区间带来的问提就是为了达到同样的精度需要增大量化区间，也就需要增大量化位宽，存储需求就增大了，凉凉。所以总是要有牺牲的，那么现在的很多研究就是在减小这个牺牲。

前面说的这种方式是直接对训练好的模型这样进行处理，又由于我们demo很小，板子空间又蛮大的，所以我们使用的是16bit量化位宽，显然我们有大把的空间可以去浪费。但是我们肯定是希望使用更小的位宽了，所以在后面我做其他项目的时候，我还是增加了`zero`进去的，解量化用的和没有`zero`同样的方式，为了减小这个误差，我又将量化后的模型在pytorch上面retrain了一下，误差减小了很多，在工程可接受范围内。当然这上面各种各样的方法有很多，因为我并不是做这方面研究的，所以只是以实现工程要求为目标，并没有过多的去考虑该如何做。

现在来考虑结合一下FPGA，FPGA最擅长的是什么呢，逻辑运算，举个例子，左移一位和乘以2，虽然得到的效果一样，但是FPGA更擅长前者。因此我们针对于在FPGA实现做进一步优化，我们需要保证`scale`是2的幂，这样一来不管是`×scale`还是`/scale`都可以使用移位来进行。

那么在FPGA如何进行量化与解量化呢？

首先权重和偏置肯定是我们量化好存进去的，输入图像也是量化好的，重点在于中间过程的量化与解量化。我们还是先回到这个简单的demo上面，不使用`zero`，而且保证`scale`是2的幂 ，实现起来非常简单，就只需要简单的移位截位进位就好。

移位不用多说，首先是截位，比如说我们的两个8bit定点数乘法的输出是一个16bit的定点数，而这时候如果我们只需要高8位，那么只需要直接截断就好。
这里注意的是进位，由于四舍五入，对于正数来说，四舍五入就是让截位后的数字+最低位后一位数字，因为对于定点数而言，我们截位的地方就是我们的小数点，小数点后面第一位如果是1，那么就以为这小数大于等于0.5，如果是0，就是小于0.5。对于负数来说，因此四舍五入的时候应该怎么处理呢？留个思考题2333

在实际的运行当中，层间的解量化与量化是连在一起的，因此可以将他们合并处理。假设第`l`层的输出数据尺度因子为`scale_l`​，那么这一层的解量化为：

f_l(x)=f_{ql}(x)/scale_l

而对于第`l+1`层的输入来说，我们需要再重新对他进行量化，这时候的尺度因此为`scale_{(l+1)_{data}}`​​，那么我们这时候的量化为：

x_q = f_l(x)*scale_{{(l+1)}_{data}}

也就是说

x_q = f_{ql}(x)\times(scale_{(l+1)_{data}}/scale_l)

而这些尺度因子都是我们在软件层面提前设计好的，在实现的时候，我们可以直接使用一次量化操作，也就是这时候的尺度因此就是`scale_{(l+1)_{data}}/scale_l`。而我们尺度因子都是2的幂，所以最后我们只需要确定新的移位方向和移动位数就好。

要注意的是，如果我们仅仅是实现前向传播，那么我们并不需要对最后的输出进行解量化，因为分类的时候我们比较的是概率输出，只要不影响相对大小关系，就不会影响最终的结果，与量化与否无关。

另外对于包含了BN层的网络来说，由于BN层的参数在训练结束后是固定的，因此可以将其融入到卷积层当中

f(x)=w\otimes x + b

f_{BN}(x)=(f(x)-{mean})/\sqrt{var}
​
融合之后得到:

f(x)=(w/\sqrt{var})\otimes x+(b-mean)/\sqrt{var}
​
这里面的`mean`和`var`都是训练集得到的固定值，所以就是得到了新的`w`和`b`。

对于离线模型的量化暂时说这么多，还有很多更复杂的方法，这不是我们简单demo的范围内，下一节将会讲解如何在训练中加入自己的量化，虽然这个过程完全使用Pytorch实现（当然也会有人使用其他框架实现），暂时彻底脱离了FPGA，但是最终得到的模型可以很easy的在FPGA上进行部署。

---

上一节我们介绍了如何对一个离线模型进行简单的线性量化，这一节我们来说一下如何在Pytorch上面训练一个量化模型。这一节可能是彻底脱离了FPGA实现，想要安心做FPGA实现的可以跳过这一节。

首先在Pytorch实现有两种情况，一种是训练的时候就是用int类型，另一种是训练的参数进行量化，但是使用float来表示。另外就是在实现过程中，现在的研究大家使用了各种各样的trick来减小量化带来的误差，从而进一步减小bit位宽，这不是我们研究的重点，真要研究起来这个领域也是非常的深奥，这里我们仅围绕如何在一个模型的训练过程里面加入量化来展开，就是用第二种方式来做例子。

首先我们先来分析一个Pytorch的卷积类。

```python
class Conv2d(_ConvNd):
    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: _size_2_t,
        stride: _size_2_t = 1,
        padding: _size_2_t = 0,
        dilation: _size_2_t = 1,
        groups: int = 1,
        bias: bool = True,
        padding_mode: str = 'zeros'  # TODO: refine this type
    ):
        kernel_size = _pair(kernel_size)
        stride = _pair(stride)
        padding = _pair(padding)
        dilation = _pair(dilation)
        super(Conv2d, self).__init__(
            in_channels, out_channels, kernel_size, stride, padding, dilation,
            False, _pair(0), groups, bias, padding_mode)

    def _conv_forward(self, input, weight):
        if self.padding_mode != 'zeros':
            return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),
                            weight, self.bias, self.stride,
                            _pair(0), self.dilation, self.groups)
        return F.conv2d(input, weight, self.bias, self.stride,
                        self.padding, self.dilation, self.groups)

    def forward(self, input: Tensor) -> Tensor:
        return self._conv_forward(input, self.weight)
```

分析一下这个类首先是__ init__方法，这一方法的作用顾名思义，就是初始化各个参数，比如输入通道数量，输出通道数量等（实际上这是一个python的特性，创建该类对象的时候会使用此初始化函数）。之后的_conv_forward方法是一个功能方法，也就是描述了这个模块的功能，实际上是调用function.py文件中的conv2d方法。最后forward用于该层全向传播的。
也就是说我们想要自己构造一个可用的，那么也需要三部分函数，一个初始化方法，一个（或者一些）功能方法，一个前向传播方法。至少从这个类看起来是这样的。
实际上这是Pytorch的一个特性，在前向传播的时候会调用所有网络中的层的forward方法来进行，因此我们在构建自己的类的时候只需要也同样的使用forward方法，在网络前向传播的时候就会自动调用。那么反向传播呢？事实上，如果我们在自己创建的类继承nn.conv2d，那么就可以继承其反向传播，来更新weight和bias。因此我们自己创建的类里面需要有这样几个要素：

-    功能方法，在这个方法里面实现量化的过程
-    forward方法，在这个方法里面实现前向传播过程
-    weight和bias变量
-    继承nn.conv2d

下面来看这段代码(要注意的是这并不是一段可执行的代码，因为疫情原因我的程序在学校的电脑上没法取出来，所以只是写了一个模板，后面会对他进行更新）：

```python
class DCconv2d(nn.Conv2d):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):
        super(DCconv2d, self).__init__(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)
        self.lamda = 2**st
        
    def quan(self,x):
    	max_data = max(x.abs())
    	scale = max_quan/max_data
        x_q = round(self.x*scale)
        return x_q
        
    def forward(self, x):
        return  F.conv2d(self.quan(x), self.quan(self.weight), self.quan(self.bias), self.stride, self.padding, self.dilation, self.groups)
```

这样一来，我们在自己创建的类当中定义了quan这一功能方法，并在forward当中调用了F.conv2d方法。而后我们在构建网络的时候使用新的类来取代nn.conv2d就好了。

在计算卷积的时候使用的是量化后的参数和数据，而在更新的时候更新的是`self.weight`和`self.bias`，对于我们这个类来说，是更新了没有量化的参数，另一种方式是使用量化后的参数对`self.weight`和`self.bias`进行覆盖，这样一来更新的也是量化后的参数，两种方式的优劣还是各位自行比较，因为有许多优化的方法在里面，通过对quan这一功能方法进行修改，来得到更优的量化效果。当然这种很简单没有优化的方法已经比仅仅对训练好的模型进行量化并部署效果要好很多了。

还是老话，这个系列的文章主要在于介绍整个流程，适用于入门新手，对于深入的研究，每个点都是一个很深奥的领域，我的研究点在于模型压缩，对于量化的方法研究的不是非常的深入，有兴趣的同学可以自行深入研究。

## 5. 数据读写

<https://blog.csdn.net/qq_38798425/article/details/107528806>

前面我们用了很大的篇幅来说如何去做前期准备，实际上，在FPGA实现CNN最难的地方也就在于前期的设计与规划，后面的Verilog部署那就是体力活+经验活了，拼排流水的功底。所以做好前期准备是非常重要的。这个系列的博文由于是为了给新手一个入门的思路，因此很多细节都没有考虑，真正的循环如何去展开如何去并行，可以看看论文，推荐一篇2017年的很经典的论文 `Optimizing Loop Operation and Dataflow in FPGA Acceleration of Deep Convolutional Neural Networks`，发表在FPGA2017，非常详细的把CNN的循环结构与循环优化进行的讲解。

首先我们来讨论数据的性质。

-    参数包括weight、bias，如果存储空间足够（比如这个demo），可以将这些做成coe文件存进片上，如果空间不足，需要先写入flash，上电后将参数存入DDR，之后的时间就从DDR读取参数。
-    输入数据两种可能，一种是在做research的时候只想看一帧的运行，那么就只需将这一帧的数据像参数一样处理，而如果想做成实时系统，输入数据有其他的输入来源比如摄像头等，就不需要一开始存进去，而是需要开辟一块儿缓存空间。
-   层间数据，这部分肯定是需要开辟缓存空间的，上一层的输出缓存进去，再读取数据作为下一层的输入。

根据数据的性质，存储模块分成这样几种：

-    只需要用于读取的模块
-    在运行过程中需要不断的写入与读取的模块
-    在初始化阶段需要写入，在运行过程中只需要读取的模块

对于第一种情况，实际上就是在片上存储空间足够的时候，我们使用COE文件将数据存储在片上BRAM中，在使用的时候去读取。这种情况我们使用BRAM例化一个单口ROM。

对于第二种情况，有两种可能，一种是我们片上资源充裕，使用BRAM例化双口RAM，另一种是调取片外DDR。

对于第三种情况，与第二种情况类似，不过是写入是一次性的，从flash写入双口RAM或者DDR当中。
对于我们这个demo，就不考虑DDR和Flash写入的问题了（这部分也不是很复杂，网上都有各种各样的教程），所有能写死的数据全都用COE文件写死，缓存也都使用BRAM来例化双口RAM。

下面给出BRAM例化为单口RAM的过程：

1、选择Block Memory Generator IP核，这个在Vivado里面应该是可以用AXI4协议的。

![选择Block Memory Generator IP core](https://img-blog.csdnimg.cn/20200723113002929.png)

2、选择单口RAM选择单口RAM

![](https://img-blog.csdnimg.cn/20200723114427306.png)

3、设置大小，这里的位宽根据每个数据的位宽大小来这顶，而深度根据数据量来设定。

![设置大小](https://img-blog.csdnimg.cn/20200723114452813.png)

4、添加COE文件，要注意的是，COE文件位宽和数据个数要与前面设置的位宽和深度相对应。

![](https://img-blog.csdnimg.cn/20200723114540854.png)

例化双口RAM的过程如下：

1、还是先选择IP核

![](https://img-blog.csdnimg.cn/20200723114831627.png)

2、选择例化为双口RAM（这里有两种双口RAM，一种是两个口都可以读写，另一种是一口读一口写，我们只需要后面这种）

![](https://img-blog.csdnimg.cn/20200723115241757.png)

3、我们可以很明确的看到这里需要设置读和写两个口的位宽，而由于他们公用同样的存储空间，仅需要在读口设置深度来确定空间大小即可。

![](https://img-blog.csdnimg.cn/20200723122142973.png)

4、同样的这里可以设置初始化的COE文件。

![](https://img-blog.csdnimg.cn/20200723122237992.png)

现在我们来考虑读写时序。

首先是按顺序读取输入数据，我们前面曾经说过这个demo给每个卷积操作只分配1个DSP单元，因此我们一次只能读取一个数据进行计算。这个很好理解，就是简单的按照卷积的顺序进行。下图给出了数据读取顺序。（当然有些人的习惯是按照列来进行，都一样的）

![](https://img-blog.csdnimg.cn/20200724145712358.jpg)

到程序里面最粗暴的写法就是行列转地址的方式，[demo在这里](https://github.com/MasLiang/CNN-On-FPGA/blob/548ee86c9174affe6c55214a0d8380e0baa16e74/verilog/pic_in.v#L98)。这种方式是设置行和列变量，通过所在的行列位置，来计算在存储空间中的地址，边界情况、时序等就比较考察verilog的功底了，这就是一个细心的体力活，不做多叙述。

下面我们来看进一步的，在前面我们说，这个demo中会使用多组卷积并行的方式。我们以第一层为例，假设是4组卷积并行，那么数据读取的顺序如下图所示：

![](https://img-blog.csdnimg.cn/20200724153424444.jpg)

我们需要同时输出4个数，可是ROM或者RAM的一个口只能在一个时钟内读取一个地址的数据，那怎么办呢~这里提供几种方法：

最简单粗暴的显然是我搞四个一毛一样的ROM或者RAM，读的时候从四个里面每个输出一个数就好了。这个时候需要算好了我们还剩下多少空间可供我们使用。
稍微复杂一些的就做两个一毛一样的ROM或者RAM，然后存储数据的时候，一个地址对应两个数，这样我可以一次性读取横着的两个数，再分割，下一次的时候，把右边的数交给左边，右边读取新的数据。

更复杂的那就是自己根据数据的读取顺序来设计存储顺序，比如卷积展开成矩阵乘法等，这样可以把离散的地址变成连续的地址，就可以在一个时钟内读取。
我们demo中使用最简单粗暴的方法，抱歉，有资源就是可以为所欲为[斜眼笑],如果资源不够多，就暂时不要去实现多组卷积并行的方式，反正目的也是熟悉整个开发流程。

输入数据是这样的，其他的层间数据也是一个道理，就不再多说。

至于参数，相对于数据更加简单，和数据是一样的读取方式，但是不需要滑动，就是这些参数一直在循环读取。

而这里最重要的是两种数据的对齐，需要同时开始读取，按照一样的顺序进行读取。

下面来分析写时序。写时序实际上是一样的道理，也是使用行列位置转地址的方式进行，只不过使用的端口变成了写入端口。

数据读写模块就到这里，实现很简单，主要在于时序的对齐。下一节将介绍卷积模块的设计。

## 6. 卷积模块

<https://blog.csdn.net/qq_38798425/article/details/108948168>

将卷积展开后要进行的运算实质上是大规模矩阵运算，因此卷积模块的实现时最容易的，什么都不需要考虑，数据按顺序来了就计算，而这个顺序是数据读取部分需要考虑的，计算完了输出去这部分是下一层的数据数据存储部分需要考虑的。因此整体而言，整个网络模型中最容易实现的却是这里面最核心的计算部分。

言归正传。首先要对卷积的循环进行分析，这也是很多基于FPGA的CNN加速器里面所重点研究的。这里推荐一篇FPGA2017的论文，对循环的优化做了比较详细的分析。我们的demo就用最简单的方式进行了。

卷积循环分为四层，这里引用上面说的论文中的伪代码图。

![](https://img-blog.csdnimg.cn/20201007104704557.png)

摘自上面说的那个论文

分别解释一下：

    Loop1：是指卷积核的xy方向计算
    Loop2：是指滤波器不同输入通道的卷积核
    Loop3：是指滤波器在特征图上面的滑动
    Loop4：是指多个滤波器

循环真正影响到的应该是数据的读取时序。循环1要做的是读取一个卷积核的参数和特征图中对应的部分进行乘加运算；循环2要做的是循环读取不同的输入通道的卷积核和特征图数据；循环3要做的是在特征图上滑动，也就是说循环的在特征图上读取以不同像素为中心的数据块；循环4要做的是循环读取不同的滤波器。而卷积模块中就是要针对于这些数据读取顺序进行相应的计算。

这部分优化的方法有很多，举个例子，对于大型的网络模型而言，大多数参数和特征图是要存在片外的，而片外访存的代价是很大的，因此尽可能的进行数据复用是非常有效的优化方式，因此有些研究对循环进行的顺序交替，来提升数据的复用效率，在此不再赘述，有兴趣的同学可以去看看17/18年的论文，对这部分的讨论已经很清晰了。

我们仅仅使用最简单的方式，也就是最原始的4层循环来进行。下面要做的就是在模块中设计一个乘法阵列，也就是很多研究中所说的PE阵列、脉动阵列之类的，都是相似的思想。

这里我们调用DSP单元用作定点数乘法器设计。注意，这里使用的是定点数乘法器，也就是说，我们前面提到的定点数量化在这里派上了用场，根据前面的量化位宽来对DSP乘法器进行设计。

这里有一个trick可以使用一个DSP进行多个定点乘法操作，详见[这里](http://ieeexplore.ieee.org/document/7927113/)。

得到的结果还需要进行位宽截断处理。举个例子。如果我们使用8bit进行量化，那么我们得到的乘法输出是16bit数据，再经过加法运算，会进一步增大位宽，假设 3 × 3 3 \times 3 3×3的卷积核，那么加法结果会变成20bit，而下一层的输入仍需要8bit位宽。这时候就需要在软件层面进行量化的时候就设计好，在这一层需要保留多少位小数位。假设需要保留4位小数位，而之前的8bit输入是3位小数位，那么结果应该是有6位小数位，也就是说我们需要去掉最后的2位（好像还是很大，这说明什么？要不然高位全是0，要不然就不可能存在3位4位小数的样子。总之这里要在软件层面量化的时候就要考虑好）。

要注意的是，我们除了卷积的乘加运算之外，还有bias 的加法运算，这里的量化也需要对小数位进行对齐。

最后，我们还可以在这一层加入激活函数，如果是ReLU就很舒服了，直接判断一下输出的符号位，是1就输出0，是0就输出原始数据，其他的也一样，只需要在流水线的最后加上一级判断就可以得到激活值。一般来说，如果使用了一些其他如sigmoid的激活函数，是要使用分段线性函数进行拟合的，这样虽然会产生一定的误差，但是如果在软件模型就使用了分段线性函数，那么模型是可以学习到这种的误差的，而使用分段线性函数，那么这里的计算就又变成了和ReLU类似的选择和乘法运算。值得注意的是，一般而言为了减少乘法数量，我们会选择使用2的幂作为分段斜率，从而将乘法转化为移位运算。

理论说完了，我们来看一下程序中是如何做的。

首先是一个基础计算单元，这个单元中可以设置卷积核的尺寸，从而得知累加需要的时钟个数。也就是说PE单元中将会完成Loop1的内容。详细见[这里](https://github.com/MasLiang/CNN-On-FPGA/blob/master/verilog/Conv_unsign_sign.v)。

在[外层模块](https://github.com/MasLiang/CNN-On-FPGA/blob/d683045c146a19ea48e8aab0f71675eae670ee02/verilog/Layer1_Conv_Top.v#L66)我们使用了generate来进行循环布线，这是verilog语法中一种比较便利的可以进行并行化布线的方式，即循环内的所有内容都会并行的进行布线。

[第一组循环](https://github.com/MasLiang/CNN-On-FPGA/blob/d683045c146a19ea48e8aab0f71675eae670ee02/verilog/Layer1_Conv_Top.v#L67)使用嵌套的方式完成了Loop2和Loop4的乘法计算内容。这里值得注意的是，存在着输入通道为1的特殊情况，如果不为1呢？可以参考[这里](https://github.com/MasLiang/CNN-On-FPGA/blob/d683045c146a19ea48e8aab0f71675eae670ee02/verilog/Layer2_Conv_Top.v#L81)。

[第二组循环](https://github.com/MasLiang/CNN-On-FPGA/blob/d683045c146a19ea48e8aab0f71675eae670ee02/verilog/Layer1_Conv_Top.v#L86)中对乘法的结果进行的顺序整合，以便于后面池化层进行池化操作。另外，这里还进行了截位处理和一次选择运算，也就是前面提到的位宽对齐和ReLU激活函数。

[第三组循环](https://github.com/MasLiang/CNN-On-FPGA/blob/d683045c146a19ea48e8aab0f71675eae670ee02/verilog/Layer1_Conv_Top.v#L104)将输出进行整合，便于模块间通信。

[第四组循环](https://github.com/MasLiang/CNN-On-FPGA/blob/d683045c146a19ea48e8aab0f71675eae670ee02/verilog/Layer1_Conv_Top.v#L112)用于将输入数据进行拆分，分散给不同的基础计算单元。

[第五组循环](https://github.com/MasLiang/CNN-On-FPGA/blob/d683045c146a19ea48e8aab0f71675eae670ee02/verilog/Layer1_Conv_Top.v#L117)用于将输入的权重进行拆分，分散给不同的基础计算单元。

整个模块可以提供了大量的parameter，可以很方便的进行配置。

细心的朋友可能会发现，这里缺少了Loop3的计算部分。这是因为我们将Loop3和Loop4进行了交换，并将Loop3归入了上一节数据读写模块当中。

这就是卷积模块的全部内容了。这部分程序设计还是蛮容易的，因为涉及到的时序问题很少，都是一些并行问题。而这部分优化主要是结合数据读写部分的优化，如循环展开、交换等，还有DSP的复用技术，定点数量化bit位宽越低，DSP可以计算的定点数乘法个数越多。

另外一种优化策略涉及到整个模型的改变，也就是使用快速算法，如`FFT`、`Winograd`算法，很多文章对此进行了研究，比如[FPGA2018的这篇](https://dl.acm.org/doi/10.1145/3174243.3174257)就使用了`Winograd`算法。这方面我没有过多的研究，感兴趣的朋友可以自己研究一下。

## 7. 池化模块、全连接模块与输出

<https://blog.csdn.net/qq_38798425/article/details/108951151>

池化模块应该是最简单的一个模块了，池化要做的就是等待4个数据的投喂，然后吐出去一个数据。这个逻辑是不是看起来很眼熟？没错，这就是卷积模块的逻辑，卷积模块其实就是等待9个数据的投喂（假设卷积核大小是3 x 3）然后吐出去一个数据（当然这个数据只是当前通道的数据并不是最终的输出，还需要做一步处理。）

所以整体的逻辑就不再赘述了，这里只说一下池化操作。池化操作有很多种，常用的就是最大值池化和均值池化。最大值池化非常适合FPGA运算，因为仅仅为逻辑运算，比较出一个最大值就好了，这里由于池化的尺寸不大，我们使用组合逻辑在一个时钟内完成池化操作。均值池化涉及到加法操作和除法操作（向右移位操作）。

最后来说说全连接模块。这里事实上就是简化版的卷积模块，因为全连接模块就是大规模矩阵相乘，只需要按顺序把数据和权重输入就好了。这里的优化策略也和卷积层一样，主要是数据复用、DSP复用和快速矩阵乘法。由于全连接层在许多比较先进的模型里面都去掉了，他可以被`globel pooling`取代，性能无下降且参数量大大下降，我们也不过多讲解，而且看到程序也能看出来写到这部分程序我已经有点暴躁了，大量的组合逻辑胡乱堆砌。事实上我并不建议在程序里面使用`FC`层，尤其是涉及到FPGA实现的网络模型，`FC`层真的会在各个方面拖指标后腿，拉胯的不行，既然能够用更easy的方式来实现，何乐而不为呢。

输出分类器我们使用的是softmax分类器，其运算函数如下所示：

f_i(x) = \frac{e^{a_i}}{\sum_{j}e^{a_j}}

有没有发现什么？单调啊！我们输出的是经过这个函数之后的结果比较大小出来的`index`对吧？不经过这个函数，大小关系也不会变化对吧？所以，这里完全不需要任何计算法，直接比较就好了。为了简单，我在程序里面没有开新的模块，直接就接在了全连接的后面。

## 8. 结束语

到这里也就把所有的模块讲解完了，这是一个粗制滥造的demo，既没有做各种优化，甚至都没有按照状态机去写，其本意是得到一个可以作为给低年级学生做效果展示的东西。所以重要的不在于参考我的程序，重要的一定是整个流程，我希望能够帮助其他人来理解做这样一个项目的流程，最主要的是，很多人跟我聊到这件事的时候，产生一种恐惧心理，觉得这应该是一个很复杂的事情，真的很复杂嘛？可能刚开始写的时候不知道从何入手，会觉得很复杂，我第一次写手写数字识别的时候，光是搭一个框架写了一个月，我甚至当时觉得遥遥无期，但是等到我紧急的赶出这个demo的时候，我只需要两周。当然如果需要用到DDR，用到Flash，完美的写出注释，写出工业级代码，按照三段式状态机去写，对代码进行资源优化等，可能还需要更长的时间，但是也不会长多少，这个事情真的不难。

我希望当有人看到了这里能够告诉自己，这东西，很简单，我能行，哪怕只有一个，不像我当年一样，在黑夜中行走，不知终点在何方。

最后附上当时的一个[讲解视频](https://www.bilibili.com/video/bv1yT4y1c7Uu)。